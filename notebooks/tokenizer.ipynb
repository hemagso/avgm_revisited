{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "The first step on almost any NLP pipeline is tokenization. Tokenization is the process through which a string of characters is chopped into a sequence of smaller parts, which are then called tokens, which are then assigned a numerical identifier that will be used by the models. This is done because Machine Learning models know absolutelly nothing about language, but are in general very good at crunching numbers.\n",
    "\n",
    " As an example imagine we had the following sentence. \n",
    "\n",
    "```\n",
    "I can't believe this. You can't touch the ground.\n",
    "```\n",
    "\n",
    "We could tokenize this text by splitting on whitespaces, getting the following sequence of tokens:\n",
    "\n",
    "```\n",
    "[\"I\", \"can't\", \"believe\", \"this.\", \"You\", \"can't\", \"touch\", \"the\", \"ground\"]\n",
    "```\n",
    "\n",
    "Which would then be associated with different numerical identifiers:\n",
    "\n",
    "```\n",
    "[1, 2, 3, 4, 5, 2, 6, 7, 8]\n",
    "```\n",
    "\n",
    "See that the number `2`, which represents the token `can't` shows up twice in the sequence. Take also notice that this simple method has some obvious shortcomings - For example, the model see `this.` as a single token since there aren't any spaces to split there. It might then create another token for `this!`, which might not be desirable as the model would not know that those are closely related. In practice tokenization methods are more complicated than simply splitting by white spaces (And in some languages such as Japanese this is not even an option as words are not separated by whitespaces at all). Most modern approaches train tokenization methods directly from data, usually through some application of information theory. In this article we will use a tokenization called Word Piece which we explain in more details below.\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Piece\n",
    "\n",
    "The Word Piece Model was first proposed by [Wu et al](https://arxiv.org/pdf/1609.08144.pdf). Initially created to solve the problem of segmenting Korean and Japanese text (Which, as I said above, are not whitespace separated languages) this method was then adopted to automatically segmenting text into sub-word units. But before going into the details of Word Piece let's delve a little bit into the motivation for the use of Sub-word units.\n",
    "\n",
    "## Why should we use sub-word units?\n",
    "\n",
    "But why would we want to use sub-word units instead of words? The answer is surprisingly simple and clever - Using subwords allow our model to learn from the internal structure of a word, and allow us to better deal with rare words. Let's illustrate these claims with an example:\n",
    "\n",
    "Image you had the following list of 10 words that need to be tokenized:\n",
    "\n",
    "`increasing surprising beautiful delicate quick increasingly surprisingly beautifully delicately quickly`\n",
    "\n",
    "Using regular word level tokenization would yield a total of 10 different tokens, one for each word. \n",
    "\n",
    "`increasing surprising beautiful delicate quick increasingly surprisingly beautifully delicately quickly`\n",
    "\n",
    "However we can easily see that these words are Adjectives and their Adverb version - In english this can usually be done by adding `ly` to a word. If on the other hand we use sub-word units our model might decide that the following tokenization is actually more useful:\n",
    "\n",
    "`increasing surprising beautiful delicate quick ##ly`\n",
    "\n",
    "These are only 6 tokens, which already is useful to us from a model size standpoint. This also make it easier for the model to learn that words such as `increasing` and `increasingly` have related meaning, and that most words ending with `##ly` function as adverbs. But where this technique really shines is in how it help us to deal with rare words. \n",
    "\n",
    "Imagine now your model if faced with a word it never saw in training: `supernaturally`. If we used whitespace tokenization we would have to treat this word as an Unknown Word (Usually represented by a catch all token UNK) and would probably not derive much meaning from it. However, if we use sub-word units we could compose this new word from two tokens that occur much more frequently: `supernatural` and `##ly`. This way, even though the model never saw the word `supernaturally` it can derives that it is an adverd that add a supernatural quality to an action. \n",
    "\n",
    "Supernatural is much more common than Supernaturally in english text, as we can see in [Google Books NGram Viewer](https://books.google.com/ngrams/graph?content=Supernatural%2C+Supernaturally&case_insensitive=on&year_start=1800&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t4%3B%2CSupernatural%3B%2Cc0%3B%2Cs0%3B%3Bsupernatural%3B%2Cc0%3B%3BSupernatural%3B%2Cc0%3B.t1%3B%2Csupernaturally%3B%2Cc0#t4%3B%2CSupernatural%3B%2Cc0%3B%2Cs0%3B%3Bsupernatural%3B%2Cc0%3B%3BSupernatural%3B%2Cc0%3B.t1%3B%2Csupernaturally%3B%2Cc0), and this situation will happen with a lot of different words. By using subword units we are able to better handle those rare words, and to derive meaning from them. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Code!\n",
    "\n",
    "To train our tokenizer we will use a library that is called, appropriately enought, tokenizers. This library contains\n",
    "efficient implementations of most modern tokenizers, including Word Piece.\n",
    "\n",
    "## Training the tokenizer\n",
    "\n",
    "In the tokenizer library a tokenizer is composed by some components:\n",
    "\n",
    "- **Normalizer:** Pre-process the text before feeding it to subsequent steps. Example of normalizations are Unicode Normalization, Lower-Casing, etc... \n",
    "- **Pre-Tokenizer:** Creates the initial candidate splitting that will then be fine-tuned by our tokenizer model. The most common one is pre-tokenizing by whitespace.\n",
    "- **Model:** The model that will actually do the tokenization.\n",
    "- **Post-Processor:** In charge of adding any extra processing needed for an specific language model (For example special tokens for classification or sentence separation).\n",
    "\n",
    "Other than that we only need to pick which special tokens our tokenizer will need to handle. For our model we utilized the following choices:\n",
    "\n",
    "- **Normalizer:** We use the *BertNormalizer*, the same one used to train the Bert language model. This normalizer replace all type of whitespace characters by the common whitespace, remove accented characters and apply lowercasing to all characters. It also add spaces around chinese characters (So that they are split by the pre-tokenizer), but that won't be necessary for our dataset.\n",
    "- **Pre-tokenizer:** We use the BertPreTokenizer. This Pre-tokenizer simply split on whitespace characters and punctuations.\n",
    "- **Model:** We use the Word Piece model.\n",
    "- **Post-Processor:** We have no need of any extra processing after the tokenization is done.\n",
    "\n",
    "Finally we let our model handle two special tokens: `<unk>`, for out of vocabulary tokens and `<pad>` for padding tokens. We train our tokenizer to produce a vocabulary of 30,000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, decoders, trainers, models, normalizers\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "VOCAB_SIZE = 30000\n",
    "SUBWORD_PREFIX = \"##\"\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=UNK_TOKEN))\n",
    "tokenizer.add_special_tokens([UNK_TOKEN, PAD_TOKEN])\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=SUBWORD_PREFIX)\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    limit_alphabet=1000,\n",
    "    special_tokens=[UNK_TOKEN, PAD_TOKEN],\n",
    "    show_progress=True,\n",
    "    continuing_subword_prefix=SUBWORD_PREFIX\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then read our data into memory, filtering only english language reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user</th><th>date</th><th>score</th><th>text</th><th>length</th><th>game</th><th>platform</th><th>language</th><th>set</th></tr><tr><td>str</td><td>date</td><td>i8</td><td>str</td><td>u32</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;doodlerman&quot;</td><td>2011-06-09</td><td>10</td><td>&quot;I&#x27;m one of tho…</td><td>1900</td><td>&quot;The Legend of …</td><td>&quot;Nintendo 64&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td></tr><tr><td>&quot;Jacody&quot;</td><td>2010-11-25</td><td>10</td><td>&quot;Anyone who giv…</td><td>768</td><td>&quot;The Legend of …</td><td>&quot;Nintendo 64&quot;</td><td>&quot;en&quot;</td><td>&quot;test&quot;</td></tr><tr><td>&quot;Kaistlin&quot;</td><td>2011-04-25</td><td>10</td><td>&quot;I won&#x27;t bore y…</td><td>176</td><td>&quot;The Legend of …</td><td>&quot;Nintendo 64&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td></tr><tr><td>&quot;SirCaestus&quot;</td><td>2011-06-12</td><td>10</td><td>&quot;Everything in …</td><td>153</td><td>&quot;The Legend of …</td><td>&quot;Nintendo 64&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td></tr><tr><td>&quot;StevenA&quot;</td><td>2010-03-21</td><td>10</td><td>&quot;This game is t…</td><td>504</td><td>&quot;The Legend of …</td><td>&quot;Nintendo 64&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌────────────┬────────────┬───────┬─────────────┬───┬─────────────┬─────────────┬──────────┬───────┐\n",
       "│ user       ┆ date       ┆ score ┆ text        ┆ … ┆ game        ┆ platform    ┆ language ┆ set   │\n",
       "│ ---        ┆ ---        ┆ ---   ┆ ---         ┆   ┆ ---         ┆ ---         ┆ ---      ┆ ---   │\n",
       "│ str        ┆ date       ┆ i8    ┆ str         ┆   ┆ str         ┆ str         ┆ str      ┆ str   │\n",
       "╞════════════╪════════════╪═══════╪═════════════╪═══╪═════════════╪═════════════╪══════════╪═══════╡\n",
       "│ doodlerman ┆ 2011-06-09 ┆ 10    ┆ I'm one of  ┆ … ┆ The Legend  ┆ Nintendo 64 ┆ en       ┆ train │\n",
       "│            ┆            ┆       ┆ those       ┆   ┆ of Zelda:   ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ people who  ┆   ┆ Ocarina of  ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ thin…       ┆   ┆ …           ┆             ┆          ┆       │\n",
       "│ Jacody     ┆ 2010-11-25 ┆ 10    ┆ Anyone who  ┆ … ┆ The Legend  ┆ Nintendo 64 ┆ en       ┆ test  │\n",
       "│            ┆            ┆       ┆ gives the   ┆   ┆ of Zelda:   ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ masterpiece ┆   ┆ Ocarina of  ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ …           ┆   ┆ …           ┆             ┆          ┆       │\n",
       "│ Kaistlin   ┆ 2011-04-25 ┆ 10    ┆ I won't     ┆ … ┆ The Legend  ┆ Nintendo 64 ┆ en       ┆ train │\n",
       "│            ┆            ┆       ┆ bore you    ┆   ┆ of Zelda:   ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ with what   ┆   ┆ Ocarina of  ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ every…      ┆   ┆ …           ┆             ┆          ┆       │\n",
       "│ SirCaestus ┆ 2011-06-12 ┆ 10    ┆ Everything  ┆ … ┆ The Legend  ┆ Nintendo 64 ┆ en       ┆ train │\n",
       "│            ┆            ┆       ┆ in OoT is   ┆   ┆ of Zelda:   ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ so near at  ┆   ┆ Ocarina of  ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ …           ┆   ┆ …           ┆             ┆          ┆       │\n",
       "│ StevenA    ┆ 2010-03-21 ┆ 10    ┆ This game   ┆ … ┆ The Legend  ┆ Nintendo 64 ┆ en       ┆ train │\n",
       "│            ┆            ┆       ┆ is the      ┆   ┆ of Zelda:   ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ highest     ┆   ┆ Ocarina of  ┆             ┆          ┆       │\n",
       "│            ┆            ┆       ┆ rated g…    ┆   ┆ …           ┆             ┆          ┆       │\n",
       "└────────────┴────────────┴───────┴─────────────┴───┴─────────────┴─────────────┴──────────┴───────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_en = pl.read_parquet(\"../data/processed/reviews.parquet\").filter(pl.col(\"language\") == \"en\")\n",
    "df_en.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally train our tokenizer from the loaded data, taking care of only using the\n",
    "training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    df_en.filter(pl.col(\"set\") == \"train\")[\"text\"].to_list(), trainer=trainer\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our tokenizer let's check how it tokenize our example sentence from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', \"'\", 't', 'believe', 'this', '.', 'you', 'can', \"'\", 't', 'touch', 'the', 'ground', '!']\n",
      "[48, 1456, 8, 59, 2654, 1394, 15, 1385, 1456, 8, 59, 3229, 1356, 3318, 2]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"I can't believe this. You can't touch the ground!\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute... I thought we went through all this trouble to get subword units, but all those tokens are actually \n",
    "complete words! What is happening here is that these words are frequent enought that it is worth for the model to keep \n",
    "them as a single token. However let's see what happen when we try to tokenize the sentence below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feast', 'your', 'eyes', 'on', 'this', 'accur', '##sed', 'nonsense', '.']\n",
      "[16936, 1509, 4223, 1396, 1394, 4506, 5264, 5829, 15]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Feast your eyes on this accursed nonsense.\")\n",
    "\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the tokenizer splitted the word `accursed` into two parts, `accur` and `#sed`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our work\n",
    "\n",
    "Now, let us save our work. First, let us tokenize all text and save that into our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user</th><th>date</th><th>score</th><th>text</th><th>length</th><th>game</th><th>platform</th><th>language</th><th>set</th><th>tokens</th><th>token_ids</th><th>n_tokens</th></tr><tr><td>str</td><td>date</td><td>i8</td><td>str</td><td>u32</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[str]</td><td>list[u16]</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Sigone&quot;</td><td>2013-05-21</td><td>10</td><td>&quot;If you loved B…</td><td>722</td><td>&quot;Borderlands 2&quot;</td><td>&quot;PC&quot;</td><td>&quot;en&quot;</td><td>&quot;val&quot;</td><td>[&quot;if&quot;, &quot;you&quot;, … &quot;d&quot;]</td><td>[1482, 1385, … 43]</td><td>9471</td></tr><tr><td>&quot;dusty0923&quot;</td><td>2020-06-21</td><td>9</td><td>&quot;This game is n…</td><td>975</td><td>&quot;The Last of Us…</td><td>&quot;PlayStation 4&quot;</td><td>&quot;en&quot;</td><td>&quot;test&quot;</td><td>[&quot;this&quot;, &quot;game&quot;, … &quot;.&quot;]</td><td>[1394, 1370, … 15]</td><td>9471</td></tr><tr><td>&quot;MegaOrca&quot;</td><td>2014-10-29</td><td>3</td><td>&quot;Disappointing.…</td><td>177</td><td>&quot;Sid Meier&#x27;s Ci…</td><td>&quot;PC&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td><td>[&quot;disappointing&quot;, &quot;.&quot;, … &quot;.&quot;]</td><td>[3115, 15, … 15]</td><td>9471</td></tr><tr><td>&quot;Luigirific&quot;</td><td>2013-11-15</td><td>9</td><td>&quot;Positive : +Ac…</td><td>459</td><td>&quot;The Wonderful …</td><td>&quot;Wii U&quot;</td><td>&quot;en&quot;</td><td>&quot;train&quot;</td><td>[&quot;positive&quot;, &quot;:&quot;, … &quot;0&quot;]</td><td>[3556, 27, … 17]</td><td>9471</td></tr><tr><td>&quot;xAtomicLink&quot;</td><td>2011-07-11</td><td>10</td><td>&quot;This is the pe…</td><td>222</td><td>&quot;Earth Defense …</td><td>&quot;Xbox 360&quot;</td><td>&quot;en&quot;</td><td>&quot;test&quot;</td><td>[&quot;this&quot;, &quot;is&quot;, … &quot;.&quot;]</td><td>[1394, 1377, … 15]</td><td>9471</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 12)\n",
       "┌─────────────┬────────────┬───────┬─────────────┬───┬───────┬─────────────┬────────────┬──────────┐\n",
       "│ user        ┆ date       ┆ score ┆ text        ┆ … ┆ set   ┆ tokens      ┆ token_ids  ┆ n_tokens │\n",
       "│ ---         ┆ ---        ┆ ---   ┆ ---         ┆   ┆ ---   ┆ ---         ┆ ---        ┆ ---      │\n",
       "│ str         ┆ date       ┆ i8    ┆ str         ┆   ┆ str   ┆ list[str]   ┆ list[u16]  ┆ u32      │\n",
       "╞═════════════╪════════════╪═══════╪═════════════╪═══╪═══════╪═════════════╪════════════╪══════════╡\n",
       "│ Sigone      ┆ 2013-05-21 ┆ 10    ┆ If you      ┆ … ┆ val   ┆ [\"if\",      ┆ [1482,     ┆ 9471     │\n",
       "│             ┆            ┆       ┆ loved Borde ┆   ┆       ┆ \"you\", …    ┆ 1385, …    ┆          │\n",
       "│             ┆            ┆       ┆ rlands, you ┆   ┆       ┆ \"d\"]        ┆ 43]        ┆          │\n",
       "│             ┆            ┆       ┆ wi…         ┆   ┆       ┆             ┆            ┆          │\n",
       "│ dusty0923   ┆ 2020-06-21 ┆ 9     ┆ This game   ┆ … ┆ test  ┆ [\"this\",    ┆ [1394,     ┆ 9471     │\n",
       "│             ┆            ┆       ┆ is not bad. ┆   ┆       ┆ \"game\", …   ┆ 1370, …    ┆          │\n",
       "│             ┆            ┆       ┆ It's a ter… ┆   ┆       ┆ \".\"]        ┆ 15]        ┆          │\n",
       "│ MegaOrca    ┆ 2014-10-29 ┆ 3     ┆ Disappointi ┆ … ┆ train ┆ [\"disappoin ┆ [3115, 15, ┆ 9471     │\n",
       "│             ┆            ┆       ┆ ng... Very  ┆   ┆       ┆ ting\", \".\", ┆ … 15]      ┆          │\n",
       "│             ┆            ┆       ┆ simple and… ┆   ┆       ┆ … \".\"]      ┆            ┆          │\n",
       "│ Luigirific  ┆ 2013-11-15 ┆ 9     ┆ Positive :  ┆ … ┆ train ┆ [\"positive\" ┆ [3556, 27, ┆ 9471     │\n",
       "│             ┆            ┆       ┆ +Action-Pac ┆   ┆       ┆ , \":\", …    ┆ … 17]      ┆          │\n",
       "│             ┆            ┆       ┆ ked +Epic … ┆   ┆       ┆ \"0\"]        ┆            ┆          │\n",
       "│ xAtomicLink ┆ 2011-07-11 ┆ 10    ┆ This is the ┆ … ┆ test  ┆ [\"this\",    ┆ [1394,     ┆ 9471     │\n",
       "│             ┆            ┆       ┆ perfect     ┆   ┆       ┆ \"is\", …     ┆ 1377, …    ┆          │\n",
       "│             ┆            ┆       ┆ game for    ┆   ┆       ┆ \".\"]        ┆ 15]        ┆          │\n",
       "│             ┆            ┆       ┆ any…        ┆   ┆       ┆             ┆            ┆          │\n",
       "└─────────────┴────────────┴───────┴─────────────┴───┴───────┴─────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_review(text: str) -> tuple[list[str], list[int]]:\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return encoded.tokens, encoded.ids\n",
    "\n",
    "# df_en = df_en.sample(frac=0.01)\n",
    "# df_en = df_en.drop([\"tokens\", \"token_ids\"])\n",
    "\n",
    "df_en = df_en.with_columns(\n",
    "    pl.col(\"text\").apply(tokenize_review).alias(\"results\")\n",
    ").with_columns([\n",
    "    pl.col(\"results\").apply(lambda results: results[0]).alias(\"tokens\"),\n",
    "    pl.col(\"results\").apply(lambda results: results[1]).cast(pl.List(pl.UInt16)).alias(\"token_ids\")\n",
    "]).drop(\"results\").with_columns(\n",
    "    pl.col(\"token_ids\").arr.lengths().alias(\"n_tokens\")\n",
    ")\n",
    "\n",
    "df_en.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to save the tokenizer itself for later usage in our development pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
